{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d612de89",
   "metadata": {},
   "source": [
    "# DimABSA Track A - Subtask 2: Dimensional Aspect Sentiment Triplet Extraction (DimASTE)\n",
    "\n",
    "## Obiettivo\n",
    "Dato un testo, estrarre tutti i **triplet (Aspect, Opinion, VA)** dove:\n",
    "- **Aspect**: termine che indica l'aspetto target\n",
    "- **Opinion**: termine che esprime il sentiment\n",
    "- **VA**: punteggio Valence-Arousal (V#A) con valori da 1.00 a 9.00\n",
    "\n",
    "## Pipeline\n",
    "1. **Estrazione Aspect/Opinion**: Uso di LLM (LLAMA) con prompt engineering\n",
    "2. **Predizione VA**: Modello encoder (BERT multilingua) per predire i valori continui\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f54238c",
   "metadata": {},
   "source": [
    "## 1. Setup e Import Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d6a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installazione delle dipendenze necessarie\n",
    "!pip install torch transformers datasets accelerate bitsandbytes scipy scikit-learn pandas numpy matplotlib seaborn tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb729ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    AutoModel\n",
    ")\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set seed per riproducibilit√†\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132486a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione HuggingFace Token per accesso ai modelli gated (es. Llama)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"‚úì Token HuggingFace caricato da Colab Secrets\")\n",
    "    \n",
    "    # Login automatico\n",
    "    from huggingface_hub import login\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úì Login HuggingFace completato!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚Ñπ Non in ambiente Colab o token non trovato\")\n",
    "    print(\"Per usare modelli gated (Llama):\")\n",
    "    print(\"  - In Colab: Aggiungi 'HF_TOKEN' nei Secrets (üîë)\")\n",
    "    print(\"  - Locale: Esegui 'huggingface-cli login' nel terminale\")\n",
    "    HF_TOKEN = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8537d",
   "metadata": {},
   "source": [
    "## 2. Analisi del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione URL per caricamento dati da GitHub\n",
    "BASE_URL = \"https://raw.githubusercontent.com/DimABSA/DimABSA2026/main/task-dataset\"\n",
    "TRACK = \"track_a\"\n",
    "SUBTASK = 2\n",
    "LANGUAGE = \"eng\"\n",
    "DOMAIN = \"restaurant\"\n",
    "\n",
    "# Costruzione URL\n",
    "TRAIN_URL = f\"{BASE_URL}/{TRACK}/subtask_{SUBTASK}/{LANGUAGE}/{LANGUAGE}_{DOMAIN}_train_alltasks.jsonl\"\n",
    "DEV_URL = f\"{BASE_URL}/{TRACK}/subtask_{SUBTASK}/{LANGUAGE}/{LANGUAGE}_{DOMAIN}_dev_task{SUBTASK}.jsonl\"\n",
    "\n",
    "print(\"üì° URL Dataset:\")\n",
    "print(f\"  Train: {TRAIN_URL}\")\n",
    "print(f\"  Dev:   {DEV_URL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0947e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_from_url(url: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Carica dati JSONL da un URL GitHub.\n",
    "    \n",
    "    Args:\n",
    "        url: URL del file JSONL\n",
    "        \n",
    "    Returns:\n",
    "        Lista di dizionari\n",
    "    \"\"\"\n",
    "    import urllib.request\n",
    "    \n",
    "    try:\n",
    "        print(f\"‚è≥ Caricamento da {url.split('/')[-1]}...\")\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            data = response.read().decode('utf-8')\n",
    "            items = [json.loads(line) for line in data.strip().split('\\n') if line.strip()]\n",
    "        print(f\"‚úì Caricati {len(items)} esempi\")\n",
    "        return items\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Errore nel caricamento da {url}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Caricamento dati\n",
    "train_data = load_jsonl_from_url(TRAIN_URL)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DATASET CARICATO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Numero di esempi nel training set: {len(train_data)}\")\n",
    "print(f\"\\nEsempio di dato:\")\n",
    "print(json.dumps(train_data[0], indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff8ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisi statistica del dataset\n",
    "def analyze_dataset(data: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"Analizza il dataset e ritorna statistiche.\"\"\"\n",
    "    stats = {\n",
    "        'total_samples': len(data),\n",
    "        'total_triplets': 0,\n",
    "        'null_aspects': 0,\n",
    "        'null_opinions': 0,\n",
    "        'avg_triplets_per_sample': 0,\n",
    "        'valence_values': [],\n",
    "        'arousal_values': [],\n",
    "        'text_lengths': []\n",
    "    }\n",
    "    \n",
    "    for item in data:\n",
    "        # Per subtask 2, dobbiamo convertire Quadruplet in Triplet (rimuovendo Category)\n",
    "        if 'Quadruplet' in item:\n",
    "            quads = item['Quadruplet']\n",
    "        else:\n",
    "            quads = item.get('Triplet', [])\n",
    "        \n",
    "        stats['total_triplets'] += len(quads)\n",
    "        stats['text_lengths'].append(len(item['Text'].split()))\n",
    "        \n",
    "        for quad in quads:\n",
    "            if quad['Aspect'] == 'NULL':\n",
    "                stats['null_aspects'] += 1\n",
    "            if quad['Opinion'] == 'NULL':\n",
    "                stats['null_opinions'] += 1\n",
    "            \n",
    "            # Estrai valori VA\n",
    "            va = quad['VA'].split('#')\n",
    "            stats['valence_values'].append(float(va[0]))\n",
    "            stats['arousal_values'].append(float(va[1]))\n",
    "    \n",
    "    stats['avg_triplets_per_sample'] = stats['total_triplets'] / stats['total_samples']\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Analisi\n",
    "train_stats = analyze_dataset(train_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STATISTICHE DATASET TRAINING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Totale campioni: {train_stats['total_samples']}\")\n",
    "print(f\"Totale triplet: {train_stats['total_triplets']}\")\n",
    "print(f\"Media triplet per campione: {train_stats['avg_triplets_per_sample']:.2f}\")\n",
    "print(f\"Aspect NULL: {train_stats['null_aspects']} ({train_stats['null_aspects']/train_stats['total_triplets']*100:.1f}%)\")\n",
    "print(f\"Opinion NULL: {train_stats['null_opinions']} ({train_stats['null_opinions']/train_stats['total_triplets']*100:.1f}%)\")\n",
    "print(f\"\\nLunghezza media testo: {np.mean(train_stats['text_lengths']):.1f} parole\")\n",
    "print(f\"\\nValence - Media: {np.mean(train_stats['valence_values']):.2f}, Std: {np.std(train_stats['valence_values']):.2f}\")\n",
    "print(f\"Arousal - Media: {np.mean(train_stats['arousal_values']):.2f}, Std: {np.std(train_stats['arousal_values']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ad854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazioni\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Distribuzione Valence\n",
    "axes[0, 0].hist(train_stats['valence_values'], bins=30, color='blue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Distribuzione Valence', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Valence (1-9)')\n",
    "axes[0, 0].set_ylabel('Frequenza')\n",
    "axes[0, 0].axvline(5.0, color='red', linestyle='--', label='Neutro (5.0)')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Distribuzione Arousal\n",
    "axes[0, 1].hist(train_stats['arousal_values'], bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('Distribuzione Arousal', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Arousal (1-9)')\n",
    "axes[0, 1].set_ylabel('Frequenza')\n",
    "axes[0, 1].axvline(5.0, color='red', linestyle='--', label='Neutro (5.0)')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Scatter plot VA\n",
    "axes[1, 0].scatter(train_stats['valence_values'], train_stats['arousal_values'], \n",
    "                   alpha=0.5, s=10)\n",
    "axes[1, 0].set_title('Spazio Valence-Arousal', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Valence')\n",
    "axes[1, 0].set_ylabel('Arousal')\n",
    "axes[1, 0].axhline(5.0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].axvline(5.0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribuzione lunghezza testi\n",
    "axes[1, 1].hist(train_stats['text_lengths'], bins=30, color='orange', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Distribuzione Lunghezza Testi', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Numero di parole')\n",
    "axes[1, 1].set_ylabel('Frequenza')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bdf83a",
   "metadata": {},
   "source": [
    "## 3. Preparazione dei Dati per Subtask 2\n",
    "\n",
    "Per Subtask 2, dobbiamo convertire i Quadruplet in Triplet (rimuovendo la Category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90392835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_triplets(data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Converte i Quadruplet in Triplet per Subtask 2.\"\"\"\n",
    "    converted_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        new_item = {\n",
    "            'ID': item['ID'],\n",
    "            'Text': item['Text'],\n",
    "            'Triplet': []\n",
    "        }\n",
    "        \n",
    "        # Se il dataset ha gi√† Quadruplet, convertiamo\n",
    "        quads = item.get('Quadruplet', item.get('Triplet', []))\n",
    "        \n",
    "        for quad in quads:\n",
    "            triplet = {\n",
    "                'Aspect': quad['Aspect'],\n",
    "                'Opinion': quad['Opinion'],\n",
    "                'VA': quad['VA']\n",
    "            }\n",
    "            new_item['Triplet'].append(triplet)\n",
    "        \n",
    "        converted_data.append(new_item)\n",
    "    \n",
    "    return converted_data\n",
    "\n",
    "# Conversione dei dati\n",
    "train_triplets = convert_to_triplets(train_data)\n",
    "print(f\"Dati convertiti in formato Triplet\")\n",
    "print(f\"\\nEsempio dopo conversione:\")\n",
    "print(json.dumps(train_triplets[2], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ed6ec",
   "metadata": {},
   "source": [
    "## 4. Dataset e DataLoader Personalizzati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f17ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TripletExample:\n",
    "    \"\"\"Classe per rappresentare un esempio di triplet.\"\"\"\n",
    "    id: str\n",
    "    text: str\n",
    "    triplets: List[Dict[str, str]]\n",
    "    \n",
    "class DimABSADataset(Dataset):\n",
    "    \"\"\"Dataset personalizzato per DimABSA Subtask 2.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict]):\n",
    "        self.examples = []\n",
    "        \n",
    "        for item in data:\n",
    "            example = TripletExample(\n",
    "                id=item['ID'],\n",
    "                text=item['Text'],\n",
    "                triplets=item['Triplet']\n",
    "            )\n",
    "            self.examples.append(example)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "# Creazione dataset\n",
    "train_dataset = DimABSADataset(train_triplets)\n",
    "print(f\"\\nDataset creato con {len(train_dataset)} esempi\")\n",
    "print(f\"\\nEsempio dal dataset:\")\n",
    "example = train_dataset[2]\n",
    "print(f\"ID: {example.id}\")\n",
    "print(f\"Text: {example.text}\")\n",
    "print(f\"Triplets: {example.triplets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d00bf5",
   "metadata": {},
   "source": [
    "## 5. Modello per Estrazione Aspect/Opinion con LLM (LLAMA)\n",
    "\n",
    "Utilizziamo un LLM come LLAMA con prompt engineering per estrarre Aspect e Opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e7d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AspectOpinionExtractor:\n",
    "    \"\"\"Estrattore di Aspect e Opinion usando LLM.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "        \"\"\"\n",
    "        Inizializza l'estrattore.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Nome del modello HuggingFace da utilizzare\n",
    "        \"\"\"\n",
    "        print(f\"Caricamento del modello {model_name}...\")\n",
    "        \n",
    "        # Carica tokenizer e modello (senza quantizzazione)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,  # Usa float16 per efficienza\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Set pad token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        print(\"Modello caricato con successo!\")\n",
    "    \n",
    "    def create_prompt(self, text: str) -> str:\n",
    "        \"\"\"Crea il prompt few-shot per l'estrazione.\"\"\"\n",
    "        prompt = f\"\"\"You are an expert in aspect-based sentiment analysis. Extract all aspect-opinion pairs from restaurant reviews.\n",
    "\n",
    "Instructions:\n",
    "- ASPECT: word/phrase indicating what is discussed (e.g., \"food\", \"service\", \"ambience\")\n",
    "- OPINION: sentiment word about the aspect (e.g., \"delicious\", \"terrible\", \"amazing\")\n",
    "- Use \"NULL\" if aspect or opinion is implicit\n",
    "- Extract ALL pairs from the text\n",
    "- Return ONLY a JSON list\n",
    "\n",
    "Examples:\n",
    "\n",
    "Text: \"The food was delicious but the service was slow.\"\n",
    "Output: [{{\"aspect\": \"food\", \"opinion\": \"delicious\"}}, {{\"aspect\": \"service\", \"opinion\": \"slow\"}}]\n",
    "\n",
    "Text: \"Great atmosphere and amazing cocktails!\"\n",
    "Output: [{{\"aspect\": \"atmosphere\", \"opinion\": \"Great\"}}, {{\"aspect\": \"cocktails\", \"opinion\": \"amazing\"}}]\n",
    "\n",
    "Text: \"Overpriced and disappointing.\"\n",
    "Output: [{{\"aspect\": \"NULL\", \"opinion\": \"Overpriced\"}}, {{\"aspect\": \"NULL\", \"opinion\": \"disappointing\"}}]\n",
    "\n",
    "Now extract from this text:\n",
    "\n",
    "Text: \"{text}\"\n",
    "Output:\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def extract(self, text: str, max_new_tokens: int = 256) -> List[Dict[str, str]]:\n",
    "        \"\"\"Estrae aspect e opinion dal testo.\"\"\"\n",
    "        prompt = self.create_prompt(text)\n",
    "        \n",
    "        # Tokenizza\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Genera\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=0.1,\n",
    "                do_sample=True,\n",
    "                top_p=0.95,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        # Decodifica\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Estrai la parte dopo \"Output:\"\n",
    "        try:\n",
    "            json_str = response.split(\"Output:\")[-1].strip()\n",
    "            # Rimuovi eventuali backticks markdown\n",
    "            json_str = json_str.replace('```json', '').replace('```', '').strip()\n",
    "            \n",
    "            # Prova a parsare il JSON\n",
    "            result = json.loads(json_str)\n",
    "            \n",
    "            # Normalizza il formato\n",
    "            if isinstance(result, list) and len(result) > 0:\n",
    "                return [{'Aspect': item.get('aspect', 'NULL'), \n",
    "                        'Opinion': item.get('opinion', 'NULL')} for item in result]\n",
    "            else:\n",
    "                return [{'Aspect': 'NULL', 'Opinion': 'NULL'}]\n",
    "        except Exception as e:\n",
    "            # Se il parsing fallisce, ritorna un triplet con valori NULL\n",
    "            print(f\"Warning: JSON parsing failed for text: {text[:50]}... Error: {e}\")\n",
    "            return [{'Aspect': 'NULL', 'Opinion': 'NULL'}]\n",
    "\n",
    "print(\"\\nNOTA: L'estrazione con LLM richiede una GPU con almeno 6GB di VRAM per Llama-3.2-3B.\")\n",
    "print(\"Assicurati di aver configurato il token HuggingFace nella cella iniziale.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a6382",
   "metadata": {},
   "source": [
    "## 6. Modello per Predizione VA (Valence-Arousal)\n",
    "\n",
    "Utilizziamo un encoder (BERT multilingua) con un head di regressione per predire i valori VA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VADataset(Dataset):\n",
    "    \"\"\"Dataset per il training del modello VA.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict], tokenizer, max_length: int = 128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Prepara gli esempi\n",
    "        self.examples = []\n",
    "        for item in data:\n",
    "            text = item['Text']\n",
    "            for triplet in item['Triplet']:\n",
    "                aspect = triplet['Aspect']\n",
    "                opinion = triplet['Opinion']\n",
    "                va = triplet['VA'].split('#')\n",
    "                valence = float(va[0])\n",
    "                arousal = float(va[1])\n",
    "                \n",
    "                # Crea input concatenato: [Text] [SEP] [Aspect] [SEP] [Opinion]\n",
    "                input_text = f\"{text} [SEP] {aspect} [SEP] {opinion}\"\n",
    "                \n",
    "                self.examples.append({\n",
    "                    'input_text': input_text,\n",
    "                    'valence': valence,\n",
    "                    'arousal': arousal\n",
    "                })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Estrai aspect, opinion e text dall'input_text memorizzato\n",
    "        # Format atteso: \"{text} [SEP] {aspect} [SEP] {opinion}\"\n",
    "        parts = example['input_text'].split(' [SEP] ')\n",
    "        if len(parts) == 3:\n",
    "            text, aspect, opinion = parts\n",
    "            # Tokenizza con formato BERT a due segmenti:\n",
    "            # [CLS] aspect: X; opinion: Y [SEP] text [SEP]\n",
    "            encoding = self.tokenizer(\n",
    "                f\"aspect: {aspect}; opinion: {opinion}\",  # Segmento A\n",
    "                text,  # Segmento B\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "        else:\n",
    "            # Fallback se formato non riconosciuto\n",
    "            encoding = self.tokenizer(\n",
    "                example['input_text'],\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'valence': torch.tensor(example['valence'], dtype=torch.float),\n",
    "            'arousal': torch.tensor(example['arousal'], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "print(\"Dataset VA definito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76957ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAPredictor(nn.Module):\n",
    "    \"\"\"Modello per la predizione di Valence e Arousal.\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder_name: str = \"bert-base-multilingual-cased\", dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder BERT\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        # MLP per predizione VA\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, 256)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 2)  # 2 output: valence e arousal\n",
    "        \n",
    "        # Sigmoid per normalizzare output in [0, 1], poi scaleremo a [1, 9]\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Encoding\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Usa il [CLS] token (primo token)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # MLP\n",
    "        x = self.dropout(cls_output)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Applica sigmoid e scala a [1, 9]\n",
    "        x = self.sigmoid(x)\n",
    "        x = 1 + x * 8  # Scala da [0, 1] a [1, 9]\n",
    "        \n",
    "        return x  # Shape: (batch_size, 2) -> [valence, arousal]\n",
    "\n",
    "print(\"Modello VA definito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff42bd9",
   "metadata": {},
   "source": [
    "## 7. Training del Modello VA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac136bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione training\n",
    "ENCODER_NAME = \"bert-base-multilingual-cased\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# Carica tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(ENCODER_NAME)\n",
    "\n",
    "# Split train/validation\n",
    "train_size = int(0.9 * len(train_triplets))\n",
    "val_size = len(train_triplets) - train_size\n",
    "train_split, val_split = torch.utils.data.random_split(\n",
    "    train_triplets, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "# Converti i subset in liste\n",
    "train_list = [train_triplets[i] for i in train_split.indices]\n",
    "val_list = [train_triplets[i] for i in val_split.indices]\n",
    "\n",
    "# Crea dataset\n",
    "train_va_dataset = VADataset(train_list, tokenizer, MAX_LENGTH)\n",
    "val_va_dataset = VADataset(val_list, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Crea DataLoader\n",
    "train_loader = DataLoader(train_va_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_va_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nDataset creati:\")\n",
    "print(f\"Training esempi: {len(train_va_dataset)}\")\n",
    "print(f\"Validation esempi: {len(val_va_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializza modello\n",
    "model = VAPredictor(encoder_name=ENCODER_NAME).to(device)\n",
    "\n",
    "# Optimizer e loss\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(f\"Modello inizializzato con {sum(p.numel() for p in model.parameters()):,} parametri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c539db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"Training per un'epoca.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        # Sposta batch su device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        valence = batch['valence'].to(device)\n",
    "        arousal = batch['arousal'].to(device)\n",
    "        \n",
    "        # Forward\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Loss separata per valence e arousal\n",
    "        targets = torch.stack([valence, arousal], dim=1)\n",
    "        loss = criterion(predictions, targets)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Valutazione del modello.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            valence = batch['valence'].to(device)\n",
    "            arousal = batch['arousal'].to(device)\n",
    "            \n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            targets = torch.stack([valence, arousal], dim=1)\n",
    "            \n",
    "            loss = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    \n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "    \n",
    "    # Calcola RMSE per VA combinati (come nella metrica ufficiale)\n",
    "    rmse_va = np.sqrt(np.mean((all_predictions - all_targets) ** 2))\n",
    "    \n",
    "    return total_loss / len(dataloader), rmse_va, all_predictions, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78596963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INIZIO TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_rmse': []\n",
    "}\n",
    "\n",
    "best_val_rmse = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_rmse, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Salva metriche\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_rmse'].append(val_rmse)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val RMSE (VA): {val_rmse:.4f}\")\n",
    "    \n",
    "    # Salva miglior modello\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        torch.save(model.state_dict(), 'best_va_model.pt')\n",
    "        print(f\"‚úì Nuovo miglior modello salvato! (RMSE: {val_rmse:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETATO\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Miglior RMSE (VA): {best_val_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37380e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza le curve di training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "axes[1].plot(history['val_rmse'], label='Val RMSE (VA)', marker='o', color='red')\n",
    "axes[1].set_title('Validation RMSE', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458864e",
   "metadata": {},
   "source": [
    "## 8. Valutazione Dettagliata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il miglior modello\n",
    "model.load_state_dict(torch.load('best_va_model.pt'))\n",
    "\n",
    "# Valutazione finale\n",
    "val_loss, val_rmse, predictions, targets = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "# Separa valence e arousal\n",
    "pred_valence = predictions[:, 0]\n",
    "pred_arousal = predictions[:, 1]\n",
    "true_valence = targets[:, 0]\n",
    "true_arousal = targets[:, 1]\n",
    "\n",
    "# Calcola RMSE separati\n",
    "rmse_valence = np.sqrt(mean_squared_error(true_valence, pred_valence))\n",
    "rmse_arousal = np.sqrt(mean_squared_error(true_arousal, pred_arousal))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RISULTATI VALUTAZIONE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"RMSE Valence: {rmse_valence:.4f}\")\n",
    "print(f\"RMSE Arousal: {rmse_arousal:.4f}\")\n",
    "print(f\"RMSE Combinato (VA): {val_rmse:.4f}\")\n",
    "\n",
    "# Calcola correlazione\n",
    "corr_valence = np.corrcoef(true_valence, pred_valence)[0, 1]\n",
    "corr_arousal = np.corrcoef(true_arousal, pred_arousal)[0, 1]\n",
    "print(f\"\\nCorrelazione Valence: {corr_valence:.4f}\")\n",
    "print(f\"Correlazione Arousal: {corr_arousal:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazioni predizioni vs target\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Valence\n",
    "axes[0].scatter(true_valence, pred_valence, alpha=0.5, s=20)\n",
    "axes[0].plot([1, 9], [1, 9], 'r--', label='Predizione Perfetta')\n",
    "axes[0].set_title(f'Valence Predictions (RMSE: {rmse_valence:.3f})', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('True Valence')\n",
    "axes[0].set_ylabel('Predicted Valence')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim(0, 10)\n",
    "axes[0].set_ylim(0, 10)\n",
    "\n",
    "# Arousal\n",
    "axes[1].scatter(true_arousal, pred_arousal, alpha=0.5, s=20, color='green')\n",
    "axes[1].plot([1, 9], [1, 9], 'r--', label='Predizione Perfetta')\n",
    "axes[1].set_title(f'Arousal Predictions (RMSE: {rmse_arousal:.3f})', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('True Arousal')\n",
    "axes[1].set_ylabel('Predicted Arousal')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(0, 10)\n",
    "axes[1].set_ylim(0, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d09e58",
   "metadata": {},
   "source": [
    "## 9. Pipeline Completa di Inferenza\n",
    "\n",
    "Combiniamo l'estrazione Aspect/Opinion con la predizione VA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aedf875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimABSAPipeline:\n",
    "    \"\"\"Pipeline completa per Subtask 2: LLM extraction + BERT VA prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_extractor, va_model, tokenizer, device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            llm_extractor: AspectOpinionExtractor per estrarre (Aspect, Opinion)\n",
    "            va_model: Modello BERT per predire VA\n",
    "            tokenizer: Tokenizer per il modello VA\n",
    "            device: Device (cuda/cpu)\n",
    "        \"\"\"\n",
    "        self.llm_extractor = llm_extractor\n",
    "        self.va_model = va_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.va_model.eval()\n",
    "    \n",
    "    def predict(self, text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Pipeline completa: estrae (Aspect, Opinion) con LLM, poi predice VA con BERT.\n",
    "        \n",
    "        Args:\n",
    "            text: Testo di input (solo testo, nessun ground truth)\n",
    "            \n",
    "        Returns:\n",
    "            Lista di triplet [{\"Aspect\": ..., \"Opinion\": ..., \"VA\": \"V#A\"}, ...]\n",
    "        \"\"\"\n",
    "        # STEP 1: Estrai TUTTE le coppie (Aspect, Opinion) con LLM few-shot prompting\n",
    "        pairs = self.llm_extractor.extract(text)\n",
    "        \n",
    "        # STEP 2: Per ogni coppia (Aspect, Opinion), predici VA con BERT\n",
    "        triplets = []\n",
    "        for pair in pairs:\n",
    "            aspect = pair['Aspect']\n",
    "            opinion = pair['Opinion']\n",
    "            \n",
    "            # Crea input con formato BERT a due segmenti:\n",
    "            # Segmento A: aspect e opinion\n",
    "            # Segmento B: testo\n",
    "            # Formato automatico BERT: [CLS] segment_a [SEP] segment_b [SEP]\n",
    "            segment_a = f\"aspect: {aspect}; opinion: {opinion}\"\n",
    "            segment_b = text\n",
    "            \n",
    "            # Tokenizza con due segmenti\n",
    "            encoding = self.tokenizer(\n",
    "                segment_a,\n",
    "                segment_b,\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            input_ids = encoding['input_ids'].to(self.device)\n",
    "            attention_mask = encoding['attention_mask'].to(self.device)\n",
    "            \n",
    "            # Predici VA con modello BERT\n",
    "            with torch.no_grad():\n",
    "                va_pred = self.va_model(input_ids, attention_mask)\n",
    "                valence = va_pred[0, 0].item()\n",
    "                arousal = va_pred[0, 1].item()\n",
    "            \n",
    "            # Arrotonda a 2 decimali e clamp a [1, 9]\n",
    "            valence = round(max(1.0, min(9.0, valence)), 2)\n",
    "            arousal = round(max(1.0, min(9.0, arousal)), 2)\n",
    "            \n",
    "            triplets.append({\n",
    "                'Aspect': aspect,\n",
    "                'Opinion': opinion,\n",
    "                'VA': f\"{valence:.2f}#{arousal:.2f}\"\n",
    "            })\n",
    "        \n",
    "        return triplets\n",
    "\n",
    "# NOTA: Prima di creare la pipeline, devi inizializzare l'AspectOpinionExtractor\n",
    "# Decommentare le righe seguenti per caricare il modello LLM:\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"INIZIALIZZAZIONE LLM EXTRACTOR\")\n",
    "# print(\"=\"*60)\n",
    "# llm_extractor = AspectOpinionExtractor(model_name=\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "# Crea pipeline completa\n",
    "# pipeline = DimABSAPipeline(\n",
    "#     llm_extractor=llm_extractor,  # LLM per estrarre (Aspect, Opinion)\n",
    "#     va_model=model,                 # BERT per predire VA\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# print(\"\\n‚úì Pipeline completa creata con successo!\")# print(\"  ‚Üí BERT: Predizione VA (Valence, Arousal)\")\n",
    "# print(\"  ‚Üí LLM: Estrazione (Aspect, Opinion) con few-shot prompting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e49b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test della pipeline (DECOMMENTARE dopo aver creato la pipeline)\n",
    "\n",
    "# test_examples = [\n",
    "#     {\n",
    "#         'text': 'The food was delicious but the service was terrible.',\n",
    "#     },\n",
    "#     {\n",
    "#         'text': 'Great atmosphere and amazing cocktails!',\n",
    "#     },\n",
    "#     {\n",
    "#         'text': 'The pizza was cold and tasteless.',\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"TEST PIPELINE - Estrazione completa con LLM + VA\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# for i, example in enumerate(test_examples, 1):\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Esempio {i}:\")\n",
    "#     print(f\"Text: {example['text']}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "    \n",
    "#     # Pipeline completa: LLM estrae (Aspect, Opinion), BERT predice VA\n",
    "#     predictions = pipeline.predict(text=example['text'])\n",
    "    \n",
    "#     print(f\"\\nTriplet estratti:\")\n",
    "#     for j, triplet in enumerate(predictions, 1):\n",
    "#         print(f\"  {j}. Aspect: '{triplet['Aspect']}'\")\n",
    "#         print(f\"     Opinion: '{triplet['Opinion']}'\")\n",
    "#         print(f\"     VA: {triplet['VA']}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è NOTA: Decommenta questa cella dopo aver inizializzato la pipeline con LLM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f467950",
   "metadata": {},
   "source": [
    "## 10. Generazione Submission File\n",
    "\n",
    "Prepariamo il file di submission nel formato richiesto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114f96bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(pipeline, test_data: List[Dict], output_file: str):\n",
    "    \"\"\"\n",
    "    Genera il file di submission con pipeline completa.\n",
    "    \n",
    "    Pipeline:\n",
    "      1. Input: (ID, Text)\n",
    "      2. LLAMA estrae tutte le coppie (Aspect, Opinion) con few-shot prompting\n",
    "      3. BERT predice VA per ogni coppia estratta\n",
    "      4. Output: Lista di triplet (Aspect, Opinion, VA)\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Pipeline di predizione (DimABSAPipeline)\n",
    "        test_data: Dati di test (solo ID e Text necessari)\n",
    "        output_file: Path del file di output\n",
    "    \"\"\"\n",
    "    submissions = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GENERAZIONE SUBMISSION - Pipeline Completa\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Input: (ID, Text)\")\n",
    "    print(f\"LLAMA ‚Üí estrae (Aspect, Opinion) con few-shot prompting\")\n",
    "    print(f\"BERT ‚Üí predice VA per ogni coppia estratta\")\n",
    "    print(f\"Output: {output_file}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for item in tqdm(test_data, desc=\"Generating predictions\"):\n",
    "        text = item['Text']\n",
    "        \n",
    "        # Pipeline completa: LLAMA estrae (Aspect, Opinion), BERT predice VA\n",
    "        triplets = pipeline.predict(text=text)\n",
    "        \n",
    "        submission = {\n",
    "            'ID': item['ID'],\n",
    "            'Triplet': triplets\n",
    "        }\n",
    "        submissions.append(submission)\n",
    "    \n",
    "    # Salva in formato JSONL\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for submission in submissions:\n",
    "            f.write(json.dumps(submission) + '\\n')\n",
    "    \n",
    "    print(f\"\\n‚úì Submission salvata in: {output_file}\")\n",
    "    print(f\"‚úì Totale predizioni: {len(submissions)}\")\n",
    "    return submissions\n",
    "\n",
    "# Esempio di utilizzo (DECOMMENTARE dopo aver creato la pipeline):\n",
    "\n",
    "# # Genera submission sul validation/dev set usando gold labels (per valutare VA)\n",
    "# output_path = \"predictions_dev_va_only.jsonl\"\n",
    "# generate_submission(\n",
    "#     pipeline=pipeline,\n",
    "#     test_data=val_list,\n",
    "#     output_file=output_path,\n",
    "#     use_gold_labels=True  # Usa gold aspect/opinion, predici solo VA\n",
    "# )\n",
    "\n",
    "# # Per submission finale su test set (estrazione completa):\n",
    "# # test_data = load_jsonl_from_url(TEST_URL)\n",
    "# # generate_submission(\n",
    "# # Carica dati dev per testare\n",
    "# dev_data = load_jsonl_from_url(DEV_URL)\n",
    "\n",
    "# # Genera submission su dev set (pipeline completa: LLM + VA)\n",
    "# generate_submission(\n",
    "#     pipeline=pipeline,\n",
    "#     test_data=dev_data,\n",
    "#     output_file=\"pred_eng_restaurant_dev.jsonl\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dff55f",
   "metadata": {},
   "source": [
    "## 11. Riepilogo e Prossimi Passi\n",
    "\n",
    "### Risultati Ottenuti:\n",
    "- ‚úÖ Dataset analizzato e processato\n",
    "- ‚úÖ Modello BERT VA trainato su ground truth (Text, Aspect, Opinion) ‚Üí VA\n",
    "- ‚úÖ Pipeline di inferenza end-to-end creata:\n",
    "  - Input: (ID, Text)\n",
    "  - LLAMA few-shot ‚Üí estrae (Aspect, Opinion)\n",
    "  - BERT ‚Üí predice VA per ogni coppia\n",
    "  - Output: triplet (Aspect, Opinion, VA)\n",
    "- ‚úÖ File di submission generato\n",
    "\n",
    "### Note Importanti:\n",
    "**Ground truth √® usato SOLO per training BERT VA, MAI per inference!**\n",
    "- Training BERT: usa (Text, Aspect, Opinion) ground truth per imparare a predire VA\n",
    "- Inference: input solo Text, LLAMA estrae tutto, BERT predice VA\n",
    "\n",
    "### Possibili Miglioramenti:\n",
    "1. **Estrazione Aspect/Opinion con LLM**: Attivare e ottimizzare l'estrazione con LLAMA\n",
    "2. **Data Augmentation**: Aumentare il dataset con tecniche di augmentation\n",
    "\n",
    "3. **Ensemble**: Combinare pi√π modelli per VA prediction- Valutare su pi√π domini (laptop, hotel, ecc.) se disponibili\n",
    "\n",
    "4. **Fine-tuning LLM**: Fare fine-tuning di LLAMA sul task specifico- Per una soluzione completa end-to-end, attivare l'LLM extractor\n",
    "\n",
    "5. **Multi-task Learning**: Trainare un modello end-to-end per aspect/opinion extraction + VA prediction- Il modello VA attuale usa i gold labels per Aspect/Opinion durante il training\n",
    "\n",
    "6. **Cross-lingual Transfer**: Usare modelli multilingua per trasferimento di conoscenza### Note:\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
