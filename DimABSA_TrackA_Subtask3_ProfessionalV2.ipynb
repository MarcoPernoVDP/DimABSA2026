{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585f6c9c",
   "metadata": {},
   "source": [
    "# üî• DimABSA Subtask 3: AOC Extraction con LoRA + Cross-Entropy Loss\n",
    "\n",
    "## üéØ Obiettivo\n",
    "Fine-tuning di **LLAMA 3.2-3B** con **LoRA** per estrarre triplet **(Aspect, Opinion, Category)** usando **cross-entropy loss** token-level.\n",
    "\n",
    "## üìä Pipeline\n",
    "```\n",
    "Input Text ‚Üí LLAMA + LoRA ‚Üí Token Generation + Logits ‚Üí Parse JSON ‚Üí Cross-Entropy Loss ‚Üí Backprop\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737cf378",
   "metadata": {},
   "source": [
    "## üì¶ 1. Setup e Installazione Pacchetti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbbc456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installazione pacchetti necessari\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = [\n",
    "    'transformers>=4.30.0',\n",
    "    'datasets',\n",
    "    'torch',\n",
    "    'accelerate',\n",
    "    'sentencepiece',\n",
    "    'huggingface-hub',\n",
    "    'peft',  # ‚Üê LoRA library\n",
    "    'tqdm',\n",
    "    'scikit-learn'\n",
    "]\n",
    "\n",
    "print(\"üì¶ Installazione pacchetti...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "    except:\n",
    "        print(f\"‚ö† Errore nell'installazione di {package}\")\n",
    "\n",
    "print(\"‚úÖ Pacchetti installati con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281905c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librerie\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers & PEFT\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf30c5",
   "metadata": {},
   "source": [
    "## üîë 2. Autenticazione HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdefe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Ottieni token da variabile d'ambiente\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úÖ Autenticato con HuggingFace!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  HF_TOKEN non trovato. Alcuni modelli potrebbero non essere accessibili.\")\n",
    "    print(\"   Imposta con: $env:HF_TOKEN='your_token' (Windows PowerShell)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ecd03e",
   "metadata": {},
   "source": [
    "## üìÅ 3. Caricamento Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione dataset\n",
    "TRACK = \"track_a\"\n",
    "SUBTASK = \"3\"\n",
    "LANGUAGE = \"eng\"\n",
    "DOMAIN = \"restaurant\"\n",
    "\n",
    "BASE_URL = \"https://raw.githubusercontent.com/DimABSA/DimABSA2026/main/task-dataset\"\n",
    "TRAIN_URL = f\"{BASE_URL}/{TRACK}/subtask_{SUBTASK}/{LANGUAGE}/{LANGUAGE}_{DOMAIN}_train_alltasks.jsonl\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä CONFIGURAZIONE DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Track:    {TRACK}\")\n",
    "print(f\"Subtask:  {SUBTASK} (AOC Extraction)\")\n",
    "print(f\"Language: {LANGUAGE}\")\n",
    "print(f\"Domain:   {DOMAIN}\")\n",
    "print(f\"URL:      {TRAIN_URL}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd9b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_from_url(url: str) -> List[Dict]:\n",
    "    \"\"\"Carica dati JSONL da URL GitHub.\"\"\"\n",
    "    import urllib.request\n",
    "    \n",
    "    print(f\"‚è≥ Caricamento da {url.split('/')[-1]}...\")\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        data = response.read().decode('utf-8')\n",
    "        items = [json.loads(line) for line in data.strip().split('\\n') if line.strip()]\n",
    "    print(f\"‚úì Caricati {len(items)} esempi\")\n",
    "    return items\n",
    "\n",
    "# Carica dataset\n",
    "train_data = load_jsonl_from_url(TRAIN_URL)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üìä DATASET CARICATO\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Numero totale di esempi: {len(train_data)}\")\n",
    "print(f\"\\nEsempio di record:\")\n",
    "print(json.dumps(train_data[0], indent=2))\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8335fc0",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è 4. Split Train/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9525900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_splits(data: List[Dict], \n",
    "                         train_ratio: float = 0.7,\n",
    "                         val_ratio: float = 0.15, \n",
    "                         test_ratio: float = 0.15,\n",
    "                         seed: int = 42):\n",
    "    \"\"\"Crea split train/val/test.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(len(data))\n",
    "    \n",
    "    n_train = int(len(data) * train_ratio)\n",
    "    n_val = int(len(data) * val_ratio)\n",
    "    \n",
    "    train_indices = indices[:n_train]\n",
    "    val_indices = indices[n_train:n_train + n_val]\n",
    "    test_indices = indices[n_train + n_val:]\n",
    "    \n",
    "    train_split = [data[i] for i in train_indices]\n",
    "    val_split = [data[i] for i in val_indices]\n",
    "    test_split = [data[i] for i in test_indices]\n",
    "    \n",
    "    return train_split, val_split, test_split\n",
    "\n",
    "# Crea split\n",
    "train_split, val_split, test_split = create_dataset_splits(train_data, seed=SEED)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÇÔ∏è  SPLIT DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Train: {len(train_split)} esempi ({len(train_split)/len(train_data)*100:.1f}%)\")\n",
    "print(f\"Val:   {len(val_split)} esempi ({len(val_split)/len(train_data)*100:.1f}%)\")\n",
    "print(f\"Test:  {len(test_split)} esempi ({len(test_split)/len(train_data)*100:.1f}%)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eebd60",
   "metadata": {},
   "source": [
    "## üìù 5. Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(text: str) -> str:\n",
    "    \"\"\"Crea prompt few-shot per estrazione AOC.\"\"\"\n",
    "    prompt = f\"\"\"### TASK\n",
    "You are an aspect-category-opinion extraction system for restaurant reviews. Extract triplets and return ONLY a valid JSON array.\n",
    "\n",
    "### RULES:\n",
    "1. ASPECT = target entity/attribute. Use \"NULL\" if implicit.\n",
    "2. CATEGORY = Entity#Attribute in UPPERCASE (e.g., \"FOOD#QUALITY\"). NEVER NULL.\n",
    "3. OPINION = sentiment word/phrase from text. Use \"NULL\" if implicit.\n",
    "4. Return ONLY the JSON array, nothing else.\n",
    "\n",
    "VALID CATEGORIES:\n",
    "- Entities: RESTAURANT, FOOD, DRINKS, AMBIENCE, SERVICE, LOCATION\n",
    "- Attributes: GENERAL, PRICES, QUALITY, STYLE_OPTIONS, MISCELLANEOUS\n",
    "\n",
    "### EXAMPLES:\n",
    "\n",
    "Text: \"the spicy tuna roll was unusually good and the rock shrimp tempura was awesome.\"\n",
    "[{{\"aspect\": \"spicy tuna roll\", \"category\": \"FOOD#QUALITY\", \"opinion\": \"unusually good\"}}, {{\"aspect\": \"rock shrimp tempura\", \"category\": \"FOOD#QUALITY\", \"opinion\": \"awesome\"}}]\n",
    "\n",
    "Text: \"we love the pink pony.\"\n",
    "[{{\"aspect\": \"pink pony\", \"category\": \"RESTAURANT#GENERAL\", \"opinion\": \"love\"}}]\n",
    "\n",
    "Text: \"the food here is rather good, but only if you like to wait for it.\"\n",
    "[{{\"aspect\": \"food\", \"category\": \"FOOD#QUALITY\", \"opinion\": \"rather good\"}}, {{\"aspect\": \"NULL\", \"category\": \"SERVICE#GENERAL\", \"opinion\": \"NULL\"}}]\n",
    "\n",
    "### INPUT\n",
    "Text: \"{text}\"\n",
    "\n",
    "### OUTPUT\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Test prompt\n",
    "test_text = \"The food was amazing but the service was slow.\"\n",
    "print(\"\\nüìù Esempio di prompt:\")\n",
    "print(\"=\"*80)\n",
    "print(create_prompt(test_text))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed9e022",
   "metadata": {},
   "source": [
    "## üîß 6. Parsing JSON (_parse_json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11aaa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_json_response(response: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Rimuove token non-JSON dall'output e parsa il risultato.\n",
    "    \n",
    "    Args:\n",
    "        response: Testo generato dal modello\n",
    "        \n",
    "    Returns:\n",
    "        Lista di dict con Aspect, Category, Opinion\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Rimuovi markdown\n",
    "        response = response.replace('```json', '').replace('```', '').strip()\n",
    "        \n",
    "        # Step 2: Trova tutti gli array JSON\n",
    "        candidates = re.findall(r'\\[[\\s\\S]*?\\]', response)\n",
    "        \n",
    "        for json_str in candidates:\n",
    "            try:\n",
    "                result = json.loads(json_str)\n",
    "                \n",
    "                if isinstance(result, list) and len(result) > 0:\n",
    "                    normalized = []\n",
    "                    seen = set()\n",
    "                    \n",
    "                    for item in result:\n",
    "                        if not isinstance(item, dict):\n",
    "                            continue\n",
    "                        \n",
    "                        aspect = str(item.get('aspect', item.get('Aspect', 'NULL'))).strip()\n",
    "                        category = str(item.get('category', item.get('Category', 'RESTAURANT#GENERAL'))).strip().upper()\n",
    "                        opinion = str(item.get('opinion', item.get('Opinion', 'NULL'))).strip()\n",
    "                        \n",
    "                        if not aspect:\n",
    "                            aspect = 'NULL'\n",
    "                        if not opinion:\n",
    "                            opinion = 'NULL'\n",
    "                        if not category or category == 'NULL':\n",
    "                            category = 'RESTAURANT#GENERAL'\n",
    "                        \n",
    "                        # Rimuovi duplicati\n",
    "                        triplet_key = (aspect.lower(), category, opinion.lower())\n",
    "                        if triplet_key not in seen:\n",
    "                            seen.add(triplet_key)\n",
    "                            normalized.append({\n",
    "                                'Aspect': aspect,\n",
    "                                'Category': category,\n",
    "                                'Opinion': opinion\n",
    "                            })\n",
    "                    \n",
    "                    if normalized:\n",
    "                        return normalized\n",
    "            \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "        \n",
    "        # Fallback\n",
    "        return [{'Aspect': 'NULL', 'Category': 'RESTAURANT#GENERAL', 'Opinion': 'NULL'}]\n",
    "    \n",
    "    except Exception:\n",
    "        return [{'Aspect': 'NULL', 'Category': 'RESTAURANT#GENERAL', 'Opinion': 'NULL'}]\n",
    "\n",
    "# Test parsing\n",
    "test_response = '[{\"aspect\": \"food\", \"category\": \"FOOD#QUALITY\", \"opinion\": \"great\"}]'\n",
    "print(\"\\nüîß Test parsing:\")\n",
    "print(f\"Input:  {test_response}\")\n",
    "print(f\"Output: {_parse_json_response(test_response)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6cd79d",
   "metadata": {},
   "source": [
    "## ü§ñ 7. Caricamento Modello con LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ü§ñ CARICAMENTO MODELLO CON LoRA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Carica tokenizer\n",
    "print(\"‚è≥ Caricamento tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"‚úì Tokenizer caricato\")\n",
    "\n",
    "# Carica modello base\n",
    "print(\"\\n‚è≥ Caricamento modello base...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"‚úì Modello base caricato\")\n",
    "\n",
    "# Configura LoRA\n",
    "print(\"\\n‚öôÔ∏è  Configurazione LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # Rank delle matrici LoRA\n",
    "    lora_alpha=32,           # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Applica LoRA al modello\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\n‚úÖ Modello LoRA pronto per il training!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300bfcaf",
   "metadata": {},
   "source": [
    "## üìä 8. Dataset Class per Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49750b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AOCDataset(Dataset):\n",
    "    \"\"\"Dataset per training AOC extraction con LoRA.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict], tokenizer, max_length: int = 512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Input: prompt con il testo\n",
    "        prompt = create_prompt(item['Text'])\n",
    "        \n",
    "        # Target: JSON output con AOC\n",
    "        target_aocs = [\n",
    "            {\n",
    "                \"aspect\": q['Aspect'],\n",
    "                \"category\": q['Category'],\n",
    "                \"opinion\": q['Opinion']\n",
    "            }\n",
    "            for q in item['Quadruplet']\n",
    "        ]\n",
    "        target_json = json.dumps(target_aocs)\n",
    "        \n",
    "        # Testo completo: prompt + target\n",
    "        full_text = prompt + target_json\n",
    "        \n",
    "        # Tokenizza\n",
    "        encoding = self.tokenizer(\n",
    "            full_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Labels: copia di input_ids, ma maschera il prompt\n",
    "        labels = encoding['input_ids'].clone()\n",
    "        \n",
    "        # Calcola lunghezza del prompt (da mascherare)\n",
    "        prompt_encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        prompt_length = prompt_encoding['input_ids'].shape[1]\n",
    "        \n",
    "        # Maschera il prompt (loss calcolata solo sul target JSON)\n",
    "        labels[:, :prompt_length] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': labels.squeeze()\n",
    "        }\n",
    "\n",
    "# Crea dataset\n",
    "print(\"\\nüìä Creazione dataset per training...\")\n",
    "train_dataset = AOCDataset(train_split, tokenizer, max_length=512)\n",
    "val_dataset = AOCDataset(val_split, tokenizer, max_length=512)\n",
    "\n",
    "print(f\"‚úì Train dataset: {len(train_dataset)} esempi\")\n",
    "print(f\"‚úì Val dataset:   {len(val_dataset)} esempi\")\n",
    "\n",
    "# Test dataset\n",
    "print(\"\\nüîç Test dataset:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"Input IDs shape:      {sample['input_ids'].shape}\")\n",
    "print(f\"Attention Mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Labels shape:         {sample['labels'].shape}\")\n",
    "print(f\"\\nPrimi 10 label: {sample['labels'][:10].tolist()}\")\n",
    "print(f\"(Note: -100 = prompt mascherato, loss non calcolata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f44985",
   "metadata": {},
   "source": [
    "## üéì 9. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fef19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = 4 * 4 = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_STEPS = 100\n",
    "LOGGING_STEPS = 50\n",
    "EVAL_STEPS = 200\n",
    "SAVE_STEPS = 500\n",
    "OUTPUT_DIR = \"./lora_checkpoints\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéì CONFIGURAZIONE TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Batch size:                {BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation:     {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Effective batch size:      {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Learning rate:             {LEARNING_RATE}\")\n",
    "print(f\"Num epochs:                {NUM_EPOCHS}\")\n",
    "print(f\"Warmup steps:              {WARMUP_STEPS}\")\n",
    "print(f\"Total training steps:      {len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * NUM_EPOCHS}\")\n",
    "print(f\"Output directory:          {OUTPUT_DIR}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1bdfee",
   "metadata": {},
   "source": [
    "## üöÄ 10. Training Loop con Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce190989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0  # Set to 0 for Windows compatibility\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Optimizer & Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training tracking\n",
    "training_stats = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ INIZIO TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nüìÖ Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Move to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        epoch_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "        \n",
    "        # Update weights ogni GRADIENT_ACCUMULATION_STEPS\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "            # Logging\n",
    "            if global_step % LOGGING_STEPS == 0:\n",
    "                avg_loss = epoch_loss / (step + 1)\n",
    "                lr = scheduler.get_last_lr()[0]\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{avg_loss:.4f}',\n",
    "                    'lr': f'{lr:.2e}'\n",
    "                })\n",
    "                training_stats['train_loss'].append(avg_loss)\n",
    "                training_stats['learning_rates'].append(lr)\n",
    "            \n",
    "            # Validation\n",
    "            if global_step % EVAL_STEPS == 0:\n",
    "                print(f\"\\n\\nüìä Validation at step {global_step}\")\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for val_batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                        val_input_ids = val_batch['input_ids'].to(device)\n",
    "                        val_attention_mask = val_batch['attention_mask'].to(device)\n",
    "                        val_labels = val_batch['labels'].to(device)\n",
    "                        \n",
    "                        val_outputs = model(\n",
    "                            input_ids=val_input_ids,\n",
    "                            attention_mask=val_attention_mask,\n",
    "                            labels=val_labels\n",
    "                        )\n",
    "                        val_loss += val_outputs.loss.item()\n",
    "                \n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "                training_stats['val_loss'].append(avg_val_loss)\n",
    "                \n",
    "                print(f\"Validation Loss: {avg_val_loss:.4f}\\n\")\n",
    "                model.train()\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if global_step % SAVE_STEPS == 0:\n",
    "                checkpoint_dir = f\"{OUTPUT_DIR}/checkpoint-{global_step}\"\n",
    "                print(f\"\\nüíæ Saving checkpoint to {checkpoint_dir}\")\n",
    "                model.save_pretrained(checkpoint_dir)\n",
    "                tokenizer.save_pretrained(checkpoint_dir)\n",
    "    \n",
    "    # End of epoch\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"\\nüìä Epoch {epoch + 1} completed. Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ TRAINING COMPLETATO!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save final model\n",
    "final_dir = f\"{OUTPUT_DIR}/final\"\n",
    "print(f\"\\nüíæ Saving final model to {final_dir}\")\n",
    "model.save_pretrained(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b422b5a",
   "metadata": {},
   "source": [
    "## üìà 11. Visualizzazione Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b661e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training loss\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(training_stats['train_loss'], label='Train Loss', color='blue', alpha=0.7)\n",
    "axes[0].set_xlabel('Step (x50)')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1].plot(training_stats['learning_rates'], label='Learning Rate', color='green', alpha=0.7)\n",
    "axes[1].set_xlabel('Step (x50)')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Training Statistics:\")\n",
    "print(f\"Final Train Loss: {training_stats['train_loss'][-1]:.4f}\")\n",
    "if training_stats['val_loss']:\n",
    "    print(f\"Final Val Loss:   {training_stats['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ece65",
   "metadata": {},
   "source": [
    "## üéØ 12. Inference e Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_aoc(text: str, model, tokenizer, max_new_tokens: int = 150):\n",
    "    \"\"\"Genera AOC per un testo usando il modello fine-tunato.\"\"\"\n",
    "    prompt = create_prompt(text)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=4096)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decodifica solo la parte generata\n",
    "    input_length = inputs['attention_mask'][0].sum().item()\n",
    "    generated_ids = outputs[0][input_length:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    # Parsa JSON\n",
    "    aocs = _parse_json_response(response)\n",
    "    \n",
    "    return aocs, response\n",
    "\n",
    "# Test su esempi\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ TEST INFERENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_texts = [\n",
    "    \"The food was amazing but the service was slow.\",\n",
    "    \"I love this restaurant! Great atmosphere and delicious pizza.\",\n",
    "    \"The prices are too high for the quality you get.\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"\\n--- Test {i} ---\")\n",
    "    print(f\"Text: {text}\")\n",
    "    \n",
    "    aocs, raw_response = generate_aoc(text, model, tokenizer)\n",
    "    \n",
    "    print(f\"\\nRaw response: {raw_response}\")\n",
    "    print(f\"\\nParsed AOCs:\")\n",
    "    for aoc in aocs:\n",
    "        print(f\"  - Aspect: '{aoc['Aspect']}', Category: '{aoc['Category']}', Opinion: '{aoc['Opinion']}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd33f65",
   "metadata": {},
   "source": [
    "## üìä 13. Evaluation su Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240682d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset: List[Dict], num_samples: int = 100):\n",
    "    \"\"\"\n",
    "    Valuta il modello su un dataset.\n",
    "    Calcola Precision, Recall, F1 per triplet matching.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìä EVALUATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Num samples: {num_samples}\")\n",
    "    \n",
    "    sample_data = dataset[:num_samples]\n",
    "    \n",
    "    total_tp = 0\n",
    "    total_pred = 0\n",
    "    total_gold = 0\n",
    "    \n",
    "    for item in tqdm(sample_data, desc=\"Evaluating\"):\n",
    "        text = item['Text']\n",
    "        gold_quadruplets = item['Quadruplet']\n",
    "        \n",
    "        # Predizione\n",
    "        pred_aocs, _ = generate_aoc(text, model, tokenizer)\n",
    "        \n",
    "        # Normalizza gold\n",
    "        gold_triplets = [\n",
    "            {\n",
    "                'aspect': q['Aspect'].strip().lower(),\n",
    "                'category': q['Category'].strip().upper(),\n",
    "                'opinion': q['Opinion'].strip().lower()\n",
    "            }\n",
    "            for q in gold_quadruplets\n",
    "        ]\n",
    "        \n",
    "        # Normalizza pred\n",
    "        pred_triplets = [\n",
    "            {\n",
    "                'aspect': p['Aspect'].strip().lower(),\n",
    "                'category': p['Category'].strip().upper(),\n",
    "                'opinion': p['Opinion'].strip().lower()\n",
    "            }\n",
    "            for p in pred_aocs\n",
    "        ]\n",
    "        \n",
    "        total_gold += len(gold_triplets)\n",
    "        total_pred += len(pred_triplets)\n",
    "        \n",
    "        # Match\n",
    "        matched_gold = set()\n",
    "        for pred in pred_triplets:\n",
    "            for j, gold in enumerate(gold_triplets):\n",
    "                if j in matched_gold:\n",
    "                    continue\n",
    "                if (pred['aspect'] == gold['aspect'] and\n",
    "                    pred['category'] == gold['category'] and\n",
    "                    pred['opinion'] == gold['opinion']):\n",
    "                    total_tp += 1\n",
    "                    matched_gold.add(j)\n",
    "                    break\n",
    "    \n",
    "    # Calcola metriche\n",
    "    precision = total_tp / total_pred if total_pred > 0 else 0\n",
    "    recall = total_tp / total_gold if total_gold > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìà RISULTATI\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "    print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "    print(f\"F1 Score:  {f1:.4f} ({f1*100:.2f}%)\")\n",
    "    print(f\"\\nTrue Positives:  {total_tp}\")\n",
    "    print(f\"Total Predicted: {total_pred}\")\n",
    "    print(f\"Total Gold:      {total_gold}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tp': total_tp,\n",
    "        'total_pred': total_pred,\n",
    "        'total_gold': total_gold\n",
    "    }\n",
    "\n",
    "# Evalua su validation set\n",
    "val_results = evaluate_model(model, tokenizer, val_split, num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a5cd6",
   "metadata": {},
   "source": [
    "## üíæ 14. Load LoRA Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per caricare un checkpoint salvato in futuro\n",
    "def load_lora_model(checkpoint_path: str, model_name: str = MODEL_NAME):\n",
    "    \"\"\"Carica un modello LoRA da checkpoint.\"\"\"\n",
    "    print(f\"\\n‚è≥ Caricamento LoRA checkpoint da {checkpoint_path}...\")\n",
    "    \n",
    "    # Carica tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "    \n",
    "    # Carica modello base\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Carica adapter LoRA\n",
    "    model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    \n",
    "    print(\"‚úÖ Modello LoRA caricato con successo!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Esempio uso:\n",
    "# model, tokenizer = load_lora_model(\"./lora_checkpoints/checkpoint-1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eede5e",
   "metadata": {},
   "source": [
    "## üéâ Fine!\n",
    "\n",
    "### üìù Summary\n",
    "\n",
    "Questo notebook implementa:\n",
    "\n",
    "1. ‚úÖ **LoRA fine-tuning** di LLAMA 3.2-3B\n",
    "2. ‚úÖ **Cross-entropy loss** token-level\n",
    "3. ‚úÖ **Masking del prompt** (loss solo sul target JSON)\n",
    "4. ‚úÖ **Training loop** custom con validation\n",
    "5. ‚úÖ **Checkpointing** per salvare/caricare modelli\n",
    "6. ‚úÖ **Evaluation** con Precision/Recall/F1\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "- Tunare hyperparameters (learning rate, batch size, LoRA rank)\n",
    "- Aumentare epochs se necessario\n",
    "- Confrontare con baseline (LLAMA zero-shot)\n",
    "- Test su test set finale\n",
    "- Considerare QLoRA (4-bit quantization) per memoria ridotta\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
